Controllability  操纵

Controllability in responsible AI refers to the ability to monitor and guide an AI system's behavior to align with human values and intent. It involves developing architectures that are controllable, so that any unintended issues can be managed and addressed.  
负责任的 AI 中的可控性是指监控和指导 AI 系统的行为以符合人类价值观和意图的能力。它涉及开发可控的架构，以便管理和解决任何意外问题。
By ensuring controllability, responsible AI can help mitigate risks, promote fairness and transparency, and ensure that AI systems benefit society as a whole. 
通过确保可控性，负责任的 AI 可以帮助降低风险，促进公平和透明，并确保 AI 系统造福整个社会。

Increased trust and reputation
提高信任和声誉

Customers are more likely to interact with AI applications, if they believe the system is fair and safe. This enhances their reputation and brand value. 
如果客户认为 AI 应用程序是公平和安全的，他们就更有可能与 AI 应用程序进行交互。这提高了他们的声誉和品牌价值。

Regulatory compliance  法规遵从性

As AI regulations emerge, companies with robust ethical AI frameworks are better positioned to comply with guidelines on data privacy, fairness, accountability, and transparency.
随着 AI 法规的出台，拥有强大的道德 AI 框架的公司可以更好地遵守有关数据隐私、公平性、问责制和透明度的准则。

AI governance provides the guiding processes and institutional mechanisms to help ensure responsibility, risk mitigation, and oversight for AI from research and development (R&D) to deployment. It encourages maximizing societal benefits while minimizing harm from AI. Governance is crucial for responsible adoption.
AI 治理提供了指导流程和制度机制，以帮助确保 AI 从研发 （R&D） 到部署的责任、风险缓解和监督。它鼓励最大限度地提高社会利益，同时最大限度地减少 AI 的危害。治理对于负责任的采用至关重要。


Explainability empowers users to verify system functionality, check for unwanted biases, increase useful human control, and place appropriate trust in AI systems. This dimension of AI promotes the responsible development and deployment of AI technology for the benefit of society. Without explainability, AI could lose public trust because of inscrutable failures.
可解释性使用户能够验证系统功能、检查不需要的偏差、增加有用的人工控制，并适当信任 AI 系统。AI 的这一维度促进了 AI 技术的负责任开发和部署，以造福社会。如果没有可解释性，AI 可能会因难以捉摸的失败而失去公众信任。

SageMaker Clarify provides purpose-built tools to gain greater insights into ML models and data based on metrics such as accuracy, robustness, toxicity, and bias to improve model quality and support responsible AI initiatives.
SageMaker Clarify 提供专门构建的工具，以根据准确性、稳健性、毒性和偏差等指标更深入地了解 ML 模型和数据，从而提高模型质量并支持负责任的 AI 计划。

Monitoring is important to maintain high-quality ML models and help ensure accurate predictions. SageMaker Model Monitor automatically detects and alerts users to inaccurate predictions from deployed models. With Amazon A2I, users can implement a human review of ML predictions when human oversight is needed.
监控对于维护高质量的 ML 模型并帮助确保准确预测非常重要。SageMaker Model Monitor 会自动检测已部署模型的不准确预测并提醒用户。借助 Amazon A2I，用户可以在需要人工监督时对 ML 预测进行人工审核。

With Model Evaluation on Amazon Bedrock, the team can evaluate, compare, and select the best foundation model for their use case. With Guardrails for Amazon Bedrock, the team can implement safeguards for their generative AI applications. It can help to filter out any harmful content.
借助 Amazon Bedrock 上的模型评估，团队可以评估、比较和选择适合其使用案例的最佳基础模型。借助 Amazon Bedrock 的 Guardrails，该团队可以为其生成式 AI 应用程序实施保护措施。它可以帮助过滤掉任何有害内容。

SageMaker Clarify is integrated with SageMaker Experiments to provide scores detailing which features contributed the most to your model prediction on a particular input for tabular, NLP, and computer vision models. For tabular datasets, SageMaker Clarify can also output an aggregated feature importance chart which provides insights into the overall prediction process of the model. These details can help determine if a particular model input has more influence than expected on overall model behavior.
SageMaker Clarify 与 SageMaker Experiments 集成，以提供分数，详细说明哪些特征对表格、NLP 和计算机视觉模型的特定输入的模型预测贡献最大。对于表格数据集，SageMaker Clarify 还可以输出聚合的特征重要性图表，该图表提供了对模型整体预测过程的见解。这些详细信息可以帮助确定特定模型输入对整体模型行为的影响是否大于预期。

Amazon SageMaker Autopilot uses tools provided by SageMaker Clarify to help provide insights into how ML models make predictions. These tools can help ML engineers, product managers, and other internal stakeholders understand model characteristics. To trust and interpret decisions made on model predictions, both consumers and regulators rely on transparency in machine learning.
Amazon SageMaker Autopilot 使用 SageMaker Clarify 提供的工具来帮助提供有关 ML 模型如何进行预测的见解。这些工具可以帮助 ML 工程师、产品经理和其他内部利益相关者了解模型特征。为了信任和解释根据模型预测做出的决策，消费者和监管机构都依赖于机器学习的透明度。

