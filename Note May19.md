Controllability  操纵

Controllability in responsible AI refers to the ability to monitor and guide an AI system's behavior to align with human values and intent. It involves developing architectures that are controllable, so that any unintended issues can be managed and addressed.  
负责任的 AI 中的可控性是指监控和指导 AI 系统的行为以符合人类价值观和意图的能力。它涉及开发可控的架构，以便管理和解决任何意外问题。
By ensuring controllability, responsible AI can help mitigate risks, promote fairness and transparency, and ensure that AI systems benefit society as a whole. 
通过确保可控性，负责任的 AI 可以帮助降低风险，促进公平和透明，并确保 AI 系统造福整个社会。

Increased trust and reputation
提高信任和声誉

Customers are more likely to interact with AI applications, if they believe the system is fair and safe. This enhances their reputation and brand value. 
如果客户认为 AI 应用程序是公平和安全的，他们就更有可能与 AI 应用程序进行交互。这提高了他们的声誉和品牌价值。

Regulatory compliance  法规遵从性

As AI regulations emerge, companies with robust ethical AI frameworks are better positioned to comply with guidelines on data privacy, fairness, accountability, and transparency.
随着 AI 法规的出台，拥有强大的道德 AI 框架的公司可以更好地遵守有关数据隐私、公平性、问责制和透明度的准则。

AI governance provides the guiding processes and institutional mechanisms to help ensure responsibility, risk mitigation, and oversight for AI from research and development (R&D) to deployment. It encourages maximizing societal benefits while minimizing harm from AI. Governance is crucial for responsible adoption.
AI 治理提供了指导流程和制度机制，以帮助确保 AI 从研发 （R&D） 到部署的责任、风险缓解和监督。它鼓励最大限度地提高社会利益，同时最大限度地减少 AI 的危害。治理对于负责任的采用至关重要。


Explainability empowers users to verify system functionality, check for unwanted biases, increase useful human control, and place appropriate trust in AI systems. This dimension of AI promotes the responsible development and deployment of AI technology for the benefit of society. Without explainability, AI could lose public trust because of inscrutable failures.
可解释性使用户能够验证系统功能、检查不需要的偏差、增加有用的人工控制，并适当信任 AI 系统。AI 的这一维度促进了 AI 技术的负责任开发和部署，以造福社会。如果没有可解释性，AI 可能会因难以捉摸的失败而失去公众信任。

SageMaker Clarify provides purpose-built tools to gain greater insights into ML models and data based on metrics such as accuracy, robustness, toxicity, and bias to improve model quality and support responsible AI initiatives.
SageMaker Clarify 提供专门构建的工具，以根据准确性、稳健性、毒性和偏差等指标更深入地了解 ML 模型和数据，从而提高模型质量并支持负责任的 AI 计划。

Monitoring is important to maintain high-quality ML models and help ensure accurate predictions. SageMaker Model Monitor automatically detects and alerts users to inaccurate predictions from deployed models. With Amazon A2I, users can implement a human review of ML predictions when human oversight is needed.
监控对于维护高质量的 ML 模型并帮助确保准确预测非常重要。SageMaker Model Monitor 会自动检测已部署模型的不准确预测并提醒用户。借助 Amazon A2I，用户可以在需要人工监督时对 ML 预测进行人工审核。

With Model Evaluation on Amazon Bedrock, the team can evaluate, compare, and select the best foundation model for their use case. With Guardrails for Amazon Bedrock, the team can implement safeguards for their generative AI applications. It can help to filter out any harmful content.
借助 Amazon Bedrock 上的模型评估，团队可以评估、比较和选择适合其使用案例的最佳基础模型。借助 Amazon Bedrock 的 Guardrails，该团队可以为其生成式 AI 应用程序实施保护措施。它可以帮助过滤掉任何有害内容。

SageMaker Clarify is integrated with SageMaker Experiments to provide scores detailing which features contributed the most to your model prediction on a particular input for tabular, NLP, and computer vision models. For tabular datasets, SageMaker Clarify can also output an aggregated feature importance chart which provides insights into the overall prediction process of the model. These details can help determine if a particular model input has more influence than expected on overall model behavior.
SageMaker Clarify 与 SageMaker Experiments 集成，以提供分数，详细说明哪些特征对表格、NLP 和计算机视觉模型的特定输入的模型预测贡献最大。对于表格数据集，SageMaker Clarify 还可以输出聚合的特征重要性图表，该图表提供了对模型整体预测过程的见解。这些详细信息可以帮助确定特定模型输入对整体模型行为的影响是否大于预期。

Amazon SageMaker Autopilot uses tools provided by SageMaker Clarify to help provide insights into how ML models make predictions. These tools can help ML engineers, product managers, and other internal stakeholders understand model characteristics. To trust and interpret decisions made on model predictions, both consumers and regulators rely on transparency in machine learning.
Amazon SageMaker Autopilot 使用 SageMaker Clarify 提供的工具来帮助提供有关 ML 模型如何进行预测的见解。这些工具可以帮助 ML 工程师、产品经理和其他内部利益相关者了解模型特征。为了信任和解释根据模型预测做出的决策，消费者和监管机构都依赖于机器学习的透明度。

The developer can use SageMaker Autopilot to provide explainable insights into how ML models make predictions.
开发人员可以使用 SageMaker Autopilot 提供有关 ML 模型如何进行预测的可解释见解。

AWS AI Service Cards provide transparency for AWS services. SageMaker Data Wrangler is used to balance data in cases of any imbalances. AWS HealthScribe is a HIPAA-eligible service to build clinical applications that automatically generate clinical notes by analyzing patient-clinician conversations.
AWS AI Service Cards 为 AWS 服务提供透明度。SageMaker Data Wrangler 用于在出现任何不平衡的情况下平衡数据。AWS HealthScribe 是一项符合 HIPAA 要求的服务，用于构建临床应用程序，通过分析患者与临床医生的对话来自动生成临床记录。

A model that provides transparency into a system so a human can explain the model’s output based on the weights and features is an example of interpretability in a model.
模型为系统提供了透明度，以便人类可以根据权重和特征解释模型的输出，这是模型可解释性的一个例子。

Design for amplified decision-making is a design principle that supports decision-makers in making decisions carefully in high-pressure environments by using technology.
放大决策设计是一种设计原则，它利用技术支持决策者在高压环境下谨慎地做出决策。

The design for unbiased decision-making principle focuses on reducing bias. 
无偏见决策原则的设计侧重于减少偏见。

The design for human and AI learning focuses on creating better AI systems. 
人类和人工智能学习的设计重点是创建更好的人工智能系统。



The end-to-end machine learning lifecycle process includes the following phases: 
端到端机器学习生命周期流程包括以下阶段：

Business goal identification 业务目标识别

ML problem framing  机器学习问题框架

Data processing (data collection, data preprocessing, and feature engineering) 数据处理（数据收集、数据预处理和特征工程）

Model development (training, tuning, and evaluation) 模型开发（训练、调整和评估）

Model deployment (inference and prediction) 模型部署（推理和预测）

Model monitoring  模型监控

Model retraining  模型再训练


尽管人工评估被认为是黄金标准，但自动化指标可以提供一种快速且可扩展的方法来评估基础模型性能。这些指标通常衡量模型输出的特定方面，例如：

Perplexity (a measure of how well the model predicts the next token) Perplexity（衡量模型预测下一个标记的程度）

BLEU score (for evaluating machine translation) BLEU 分数（用于评估机器翻译）

F1 score (for evaluating classification or entity recognition tasks) F1 分数（用于评估分类或实体识别任务）

Metrics like ROUGE, BLEU, and BERTScore provide an initial assessment of the foundation model's capabilities. ROUGE、BLEU 和 BERTScore 等指标提供了对基础模型功能的初步评估。

Recall-Oriented Understudy for Gisting Evaluation (ROUGE) is a set of metrics used for evaluating automatic summarization and machine translation systems. It measures the quality of a generated summary or translation by comparing it to one or more reference summaries or translations.
用于 Gisting Evaluation 的 Recall-Oriented Understudy （ROUGE） 是一组用于评估自动摘要和机器翻译系统的指标。它通过与一个或多个参考摘要或翻译进行比较来衡量生成的摘要或翻译的质量。

Bilingual Evaluation Understudy (BLEU) is a metric used to evaluate the quality of machine-generated text, particularly in the context of machine translation. It measures the similarity between a generated text and one or more reference translations, considering both precision and brevity.
双语评估替补 （BLEU） 是用于评估机器生成文本质量的指标，尤其是在机器翻译的背景下。它衡量生成的文本与一个或多个参考翻译之间的相似性，同时考虑精度和简洁性。

BERTScore is a metric that evaluates the semantic similarity between a generated text and one or more reference texts. It uses pre-trained Bidirectional Encoder Representations from Transformers (BERT) models to compute contextualized embeddings for the input texts, and then calculates the cosine similarity between them.
BERTScore 是一个指标，用于评估生成的文本与一个或多个参考文本之间的语义相似性。它使用预先训练的 Bidirectional Encoder Representations from Transformers （BERT） 模型来计算输入文本的上下文化嵌入，然后计算它们之间的余弦相似度。



